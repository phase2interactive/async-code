diff --git a/server-e2b/config.py b/server-e2b/config.py
index 7823a5d..4345177 100644
--- a/server-e2b/config.py
+++ b/server-e2b/config.py
@@ -1,19 +1,10 @@
-"""Configuration module for container security settings."""
+"""Configuration module - minimal backward compatibility wrapper."""
 # Import from centralized configuration
 from env_config import Config
 
-# Re-export container configuration for backward compatibility
-CONTAINER_UID = Config.CONTAINER_UID
-CONTAINER_GID = Config.CONTAINER_GID
-CONTAINER_USER = Config.CONTAINER_USER
-CONTAINER_SECURITY_OPTS = Config.CONTAINER_SECURITY_OPTS
-CONTAINER_READ_ONLY = Config.CONTAINER_READ_ONLY
-CONTAINER_MEM_LIMIT = Config.CONTAINER_MEM_LIMIT
-CONTAINER_CPU_SHARES = Config.CONTAINER_CPU_SHARES
-WORKSPACE_BASE_PATH = Config.WORKSPACE_BASE_PATH
-WORKSPACE_PREFIX = Config.WORKSPACE_PREFIX
-
-# Re-export functions for backward compatibility
-get_container_user_mapping = Config.get_container_user_mapping
-get_workspace_path = Config.get_workspace_path
-get_security_options = Config.get_security_options
\ No newline at end of file
+# Re-export only E2B and API configurations for backward compatibility
+# Docker-related configurations have been removed as they are no longer used
+E2B_API_KEY = Config.E2B_API_KEY
+E2B_TEMPLATE_ID = Config.E2B_TEMPLATE_ID
+ANTHROPIC_API_KEY = Config.ANTHROPIC_API_KEY
+OPENAI_API_KEY = Config.OPENAI_API_KEY
\ No newline at end of file
diff --git a/server-e2b/env_config.py b/server-e2b/env_config.py
index 5109eb8..66cb8a5 100644
--- a/server-e2b/env_config.py
+++ b/server-e2b/env_config.py
@@ -29,20 +29,9 @@ class Config:
     OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
     GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
     
-    # Container Configuration
-    CONTAINER_UID = int(os.getenv('CONTAINER_UID', '1000'))
-    CONTAINER_GID = int(os.getenv('CONTAINER_GID', '1000'))
-    CONTAINER_USER = f"{CONTAINER_UID}:{CONTAINER_GID}"
-    CONTAINER_MEM_LIMIT = os.getenv('CONTAINER_MEM_LIMIT', '2g')
-    CONTAINER_CPU_SHARES = int(os.getenv('CONTAINER_CPU_SHARES', '1024'))
-    
-    # Security Configuration
-    CONTAINER_SECURITY_OPTS = ['no-new-privileges=true']
-    CONTAINER_READ_ONLY = os.getenv('CONTAINER_READ_ONLY', 'false').lower() == 'true'
-    
-    # Workspace Configuration
-    WORKSPACE_BASE_PATH = os.getenv('WORKSPACE_BASE_PATH', '/tmp')
-    WORKSPACE_PREFIX = 'ai-workspace-'
+    # E2B Configuration
+    E2B_API_KEY = os.getenv('E2B_API_KEY')
+    E2B_TEMPLATE_ID = os.getenv('E2B_TEMPLATE_ID')
     
     # Application Configuration
     FLASK_ENV = os.getenv('FLASK_ENV', 'production')
@@ -66,20 +55,6 @@ class Config:
         if missing_vars:
             raise ValueError(f"Missing required environment variables: {', '.join(missing_vars)}")
     
-    @classmethod
-    def get_container_user_mapping(cls):
-        """Get the user mapping for containers."""
-        return cls.CONTAINER_USER
-    
-    @classmethod
-    def get_workspace_path(cls, task_id):
-        """Get the workspace path for a specific task."""
-        return os.path.join(cls.WORKSPACE_BASE_PATH, f"{cls.WORKSPACE_PREFIX}{task_id}")
-    
-    @classmethod
-    def get_security_options(cls):
-        """Get the security options for containers."""
-        return cls.CONTAINER_SECURITY_OPTS.copy()
 
 
 # Only validate required environment variables if not in test mode
diff --git a/server-e2b/tasks.py b/server-e2b/tasks.py
index 6345913..30d913a 100644
--- a/server-e2b/tasks.py
+++ b/server-e2b/tasks.py
@@ -58,10 +58,10 @@ def start_task():
         if not task:
             return jsonify({'error': 'Failed to create task'}), 500
         
-        # Start task in background thread with all required parameters
+        # Start task in background thread with minimal required parameters
         thread = threading.Thread(
             target=run_ai_code_task_e2b, 
-            args=(task['id'], user_id, github_token, repo_url, branch, prompt, model, project_id)
+            args=(task['id'], user_id, github_token)
         )
         thread.daemon = True
         thread.start()
diff --git a/server-e2b/utils/__init__.py b/server-e2b/utils/__init__.py
index 131b033..2cb3c60 100644
--- a/server-e2b/utils/__init__.py
+++ b/server-e2b/utils/__init__.py
@@ -1,8 +1,7 @@
+"""
+Utils module for AI code task execution.
+"""
 import logging
-import threading
-import fcntl
-import queue
-import atexit
 
 # Import E2B implementation
 from .code_task_e2b import run_ai_code_task_e2b
@@ -11,80 +10,7 @@ from .code_task_e2b import run_ai_code_task_e2b
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
-# For backward compatibility, we'll keep the queue structure for Codex tasks
-# but it will use E2B sandboxes instead of Docker containers
-
-# Global Codex execution queue and lock for sequential processing
-codex_execution_queue = queue.Queue()
-codex_execution_lock = threading.Lock()
-codex_worker_thread = None
-codex_lock_file = '/tmp/codex_global_lock'
-
-def init_codex_sequential_processor():
-    """Initialize the sequential Codex processor"""
-    global codex_worker_thread
-    
-    def codex_worker():
-        """Worker thread that processes Codex tasks sequentially"""
-        logger.info("üîÑ Codex sequential worker thread started")
-        
-        while True:
-            try:
-                # Get the next task from the queue (blocks if empty)
-                task_data = codex_execution_queue.get(timeout=1.0)
-                if task_data is None:  # Poison pill to stop the thread
-                    logger.info("üõë Codex worker thread stopping")
-                    break
-                    
-                task_id, user_id, github_token = task_data
-                logger.info(f"üéØ Processing Codex task {task_id} sequentially")
-                
-                # Acquire file-based lock for additional safety
-                try:
-                    with open(codex_lock_file, 'w') as lock_file:
-                        fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)
-                        logger.info(f"üîí Global Codex lock acquired for task {task_id}")
-                        
-                        # Execute the task using E2B
-                        run_ai_code_task_e2b(task_id, user_id, github_token)
-                            
-                        logger.info(f"‚úÖ Codex task {task_id} completed")
-                        
-                except Exception as e:
-                    logger.error(f"‚ùå Error executing Codex task {task_id}: {e}")
-                finally:
-                    codex_execution_queue.task_done()
-                    
-            except queue.Empty:
-                continue
-            except Exception as e:
-                logger.error(f"‚ùå Error in Codex worker thread: {e}")
-                
-    # Start the worker thread if not already running
-    with codex_execution_lock:
-        if codex_worker_thread is None or not codex_worker_thread.is_alive():
-            codex_worker_thread = threading.Thread(target=codex_worker, daemon=True)
-            codex_worker_thread.start()
-            logger.info("üöÄ Codex sequential processor initialized")
-
-def queue_codex_task(task_id, user_id, github_token):
-    """Queue a Codex task for sequential execution"""
-    init_codex_sequential_processor()
-    
-    logger.info(f"üìã Queuing Codex task {task_id} for sequential execution")
-    codex_execution_queue.put((task_id, user_id, github_token))
-    
-    # Wait for the task to be processed
-    logger.info(f"‚è≥ Waiting for Codex task {task_id} to be processed...")
-    codex_execution_queue.join()
-
-# Cleanup function to stop the worker thread
-def cleanup_codex_processor():
-    """Clean up the Codex processor on exit"""
-    global codex_worker_thread
-    if codex_worker_thread and codex_worker_thread.is_alive():
-        logger.info("üßπ Shutting down Codex sequential processor")
-        codex_execution_queue.put(None)  # Poison pill
-        codex_worker_thread.join(timeout=5.0)
-
-atexit.register(cleanup_codex_processor)
\ No newline at end of file
+# Note: The codex_execution_queue has been removed as it's no longer needed with E2B.
+# E2B sandboxes are isolated and can run concurrently without resource conflicts.
+# If rate limiting is needed for API calls, it should be implemented at the API
+# client level rather than queueing entire task executions.
\ No newline at end of file
diff --git a/server-e2b/utils/agent_scripts/README.md b/server-e2b/utils/agent_scripts/README.md
new file mode 100644
index 0000000..07bd535
--- /dev/null
+++ b/server-e2b/utils/agent_scripts/README.md
@@ -0,0 +1,52 @@
+# Agent Scripts
+
+This directory contains sophisticated agent scripts that are uploaded to E2B sandboxes for execution.
+
+## Overview
+
+Instead of generating agent code inline, we maintain reusable scripts here that can:
+- Be tested independently
+- Handle complex scenarios
+- Provide better error handling
+- Be versioned and improved over time
+
+## Scripts
+
+### codex_agent.py
+
+A sophisticated GPT/Codex agent that:
+- Analyzes repository structure for context
+- Generates appropriate system prompts
+- Handles API errors gracefully
+- Supports configuration via environment variables
+- Reads prompts from files (avoiding injection issues)
+
+## Usage
+
+The scripts are automatically uploaded to E2B sandboxes when needed. The main code in `code_task_e2b_real.py` reads these scripts and uploads them to the sandbox filesystem before execution.
+
+## Adding New Agents
+
+To add a new agent:
+
+1. Create a new Python script in this directory
+2. Follow the pattern of reading configuration from environment variables
+3. Read the task prompt from `/tmp/agent_prompt.txt`
+4. Output results to stdout
+5. Update the corresponding method in `code_task_e2b_real.py`
+
+## Environment Variables
+
+Agents should read configuration from environment variables:
+- `OPENAI_API_KEY` - OpenAI API key
+- `ANTHROPIC_API_KEY` - Anthropic API key
+- `GPT_MODEL` - Model to use (default: gpt-4)
+- `MAX_TOKENS` - Maximum tokens for response
+- `TEMPERATURE` - Temperature for generation
+
+## Security
+
+- Always read prompts from files, never from command line arguments
+- Validate all inputs
+- Handle errors gracefully
+- Don't expose sensitive information in error messages
\ No newline at end of file
diff --git a/server-e2b/utils/agent_scripts/codex_agent.py b/server-e2b/utils/agent_scripts/codex_agent.py
new file mode 100644
index 0000000..4ec0e16
--- /dev/null
+++ b/server-e2b/utils/agent_scripts/codex_agent.py
@@ -0,0 +1,194 @@
+#!/usr/bin/env python3
+"""
+Codex/GPT Agent Script for E2B Sandbox Execution.
+
+This script is uploaded to the E2B sandbox and executed to run GPT-based
+code generation tasks. It reads configuration from environment variables
+and the task prompt from a file.
+"""
+import os
+import sys
+import json
+import logging
+from typing import Dict, List, Optional
+
+# Configure logging
+logging.basicConfig(
+    level=logging.INFO,
+    format='%(asctime)s - %(levelname)s - %(message)s'
+)
+logger = logging.getLogger(__name__)
+
+try:
+    import openai
+except ImportError:
+    logger.error("OpenAI library not found. Please install it with: pip install openai")
+    sys.exit(1)
+
+
+class CodexAgent:
+    """Handles GPT-based code generation tasks."""
+    
+    def __init__(self):
+        self.api_key = os.getenv("OPENAI_API_KEY")
+        if not self.api_key:
+            raise ValueError("OPENAI_API_KEY environment variable not set")
+        
+        # Configure OpenAI
+        openai.api_key = self.api_key
+        
+        # Configuration
+        self.model = os.getenv("GPT_MODEL", "gpt-4")
+        self.max_tokens = int(os.getenv("MAX_TOKENS", "2000"))
+        self.temperature = float(os.getenv("TEMPERATURE", "0.7"))
+        
+    def read_prompt(self, prompt_file: str = "/tmp/agent_prompt.txt") -> str:
+        """Read the task prompt from a file."""
+        try:
+            with open(prompt_file, 'r') as f:
+                return f.read().strip()
+        except FileNotFoundError:
+            logger.error(f"Prompt file not found: {prompt_file}")
+            raise
+        except Exception as e:
+            logger.error(f"Error reading prompt file: {e}")
+            raise
+    
+    def analyze_repository(self) -> Dict[str, List[str]]:
+        """Analyze the repository structure to provide context."""
+        repo_info = {
+            "files": [],
+            "directories": [],
+            "languages": set()
+        }
+        
+        try:
+            for root, dirs, files in os.walk("/workspace/repo"):
+                # Skip hidden directories
+                dirs[:] = [d for d in dirs if not d.startswith('.')]
+                
+                for file in files:
+                    if not file.startswith('.'):
+                        file_path = os.path.join(root, file)
+                        relative_path = os.path.relpath(file_path, "/workspace/repo")
+                        repo_info["files"].append(relative_path)
+                        
+                        # Detect language by extension
+                        ext = os.path.splitext(file)[1].lower()
+                        if ext in ['.py', '.js', '.ts', '.java', '.cpp', '.c', '.go', '.rs']:
+                            repo_info["languages"].add(ext[1:])
+                
+                for dir_name in dirs:
+                    dir_path = os.path.join(root, dir_name)
+                    relative_path = os.path.relpath(dir_path, "/workspace/repo")
+                    repo_info["directories"].append(relative_path)
+        
+        except Exception as e:
+            logger.warning(f"Error analyzing repository: {e}")
+        
+        repo_info["languages"] = list(repo_info["languages"])
+        return repo_info
+    
+    def generate_system_prompt(self, repo_info: Dict) -> str:
+        """Generate a system prompt with repository context."""
+        languages = ", ".join(repo_info["languages"]) if repo_info["languages"] else "unknown"
+        file_count = len(repo_info["files"])
+        
+        return f"""You are an expert coding assistant working on a {languages} project.
+The repository contains {file_count} files. You have full access to read and modify any file.
+
+Your task is to implement the requested changes following these guidelines:
+1. Write clean, idiomatic code that matches the existing style
+2. Add appropriate error handling and validation
+3. Include necessary imports and dependencies
+4. Ensure backward compatibility unless breaking changes are explicitly requested
+5. Add comments for complex logic
+6. Follow the project's existing patterns and conventions
+
+After making changes, provide a clear summary of what was modified and why."""
+    
+    def execute_task(self, prompt: str) -> str:
+        """Execute the code generation task using GPT."""
+        try:
+            # Analyze repository for context
+            repo_info = self.analyze_repository()
+            system_prompt = self.generate_system_prompt(repo_info)
+            
+            # Add file list to user prompt for better context
+            enhanced_prompt = f"{prompt}\n\nRepository structure:\n"
+            enhanced_prompt += f"Languages detected: {', '.join(repo_info['languages'])}\n"
+            enhanced_prompt += f"Total files: {len(repo_info['files'])}\n"
+            
+            # Include some key files in context
+            key_files = [f for f in repo_info['files'] 
+                        if any(name in f.lower() for name in ['readme', 'package.json', 'requirements.txt', 'main', 'index'])]
+            if key_files:
+                enhanced_prompt += f"Key files: {', '.join(key_files[:5])}\n"
+            
+            logger.info(f"Executing task with model: {self.model}")
+            
+            # Make API call
+            response = openai.ChatCompletion.create(
+                model=self.model,
+                messages=[
+                    {"role": "system", "content": system_prompt},
+                    {"role": "user", "content": enhanced_prompt}
+                ],
+                max_tokens=self.max_tokens,
+                temperature=self.temperature
+            )
+            
+            return response.choices[0].message.content
+            
+        except openai.error.RateLimitError:
+            logger.error("OpenAI API rate limit exceeded")
+            return "Error: API rate limit exceeded. Please try again later."
+        except openai.error.AuthenticationError:
+            logger.error("OpenAI API authentication failed")
+            return "Error: Invalid API key"
+        except Exception as e:
+            logger.error(f"Error executing task: {e}")
+            return f"Error: {str(e)}"
+    
+    def apply_changes(self, instructions: str):
+        """
+        Parse the GPT response and apply file changes.
+        This is a simple implementation - could be enhanced with
+        better parsing of code blocks and file paths.
+        """
+        logger.info("Analyzing GPT response for file changes...")
+        
+        # This is a placeholder for more sophisticated parsing
+        # In practice, you might want to:
+        # 1. Parse markdown code blocks with file paths
+        # 2. Use GPT to generate structured output (JSON)
+        # 3. Implement a more robust change detection system
+        
+        # For now, we'll just log the instructions
+        logger.info("GPT Response:")
+        print(instructions)
+
+
+def main():
+    """Main entry point for the Codex agent."""
+    try:
+        agent = CodexAgent()
+        
+        # Read prompt
+        prompt = agent.read_prompt()
+        logger.info(f"Task prompt: {prompt[:100]}...")
+        
+        # Execute task
+        result = agent.execute_task(prompt)
+        
+        # Apply changes (currently just prints)
+        agent.apply_changes(result)
+        
+    except Exception as e:
+        logger.error(f"Agent execution failed: {e}")
+        print(f"Error: {str(e)}")
+        sys.exit(1)
+
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
diff --git a/server-e2b/utils/code_task_e2b.py b/server-e2b/utils/code_task_e2b.py
index 81d5c84..5a11c53 100644
--- a/server-e2b/utils/code_task_e2b.py
+++ b/server-e2b/utils/code_task_e2b.py
@@ -12,6 +12,7 @@ import subprocess
 import tempfile
 
 from database import DatabaseOperations
+from .git_utils import parse_file_changes
 
 logger = logging.getLogger(__name__)
 
@@ -227,49 +228,3 @@ def simulate_ai_execution(workspace_dir: str, prompt: str, agent: str) -> Dict[s
         }
 
 
-def parse_file_changes(git_diff: str) -> List[Dict[str, Any]]:
-    """Parse git diff to extract individual file changes"""
-    file_changes = []
-    current_file = None
-    before_lines = []
-    after_lines = []
-    in_diff = False
-    
-    for line in git_diff.split('\n'):
-        if line.startswith('diff --git'):
-            # Save previous file if exists
-            if current_file:
-                file_changes.append({
-                    "path": current_file,
-                    "before": '\n'.join(before_lines),
-                    "after": '\n'.join(after_lines)
-                })
-            
-            # Extract filename
-            parts = line.split(' ')
-            if len(parts) >= 4:
-                current_file = parts[3][2:] if parts[3].startswith('b/') else parts[3]
-            before_lines = []
-            after_lines = []
-            in_diff = False
-            
-        elif line.startswith('@@'):
-            in_diff = True
-        elif in_diff and current_file:
-            if line.startswith('-') and not line.startswith('---'):
-                before_lines.append(line[1:])
-            elif line.startswith('+') and not line.startswith('+++'):
-                after_lines.append(line[1:])
-            elif not line.startswith('\\'):
-                before_lines.append(line[1:] if line else '')
-                after_lines.append(line[1:] if line else '')
-    
-    # Save last file
-    if current_file:
-        file_changes.append({
-            "path": current_file,
-            "before": '\n'.join(before_lines),
-            "after": '\n'.join(after_lines)
-        })
-    
-    return file_changes
\ No newline at end of file
diff --git a/server-e2b/utils/code_task_e2b_real.py b/server-e2b/utils/code_task_e2b_real.py
index 067efbd..6393e8e 100644
--- a/server-e2b/utils/code_task_e2b_real.py
+++ b/server-e2b/utils/code_task_e2b_real.py
@@ -14,6 +14,7 @@ from database import DatabaseOperations
 from models import TaskStatus
 import subprocess
 from .async_runner import run_async_task
+from .git_utils import parse_file_changes
 
 logger = logging.getLogger(__name__)
 
@@ -330,8 +331,20 @@ class E2BCodeExecutor:
     
     async def _run_codex_agent(self, sandbox: Sandbox, prompt: str) -> Dict:
         """Run Codex/GPT agent in the sandbox"""
-        # Create a Python script to run OpenAI
-        script = f'''
+        # Read the sophisticated agent script
+        agent_script_path = os.path.join(
+            os.path.dirname(__file__), 
+            'agent_scripts', 
+            'codex_agent.py'
+        )
+        
+        # Use the sophisticated script if it exists, otherwise fall back to simple version
+        if os.path.exists(agent_script_path):
+            with open(agent_script_path, 'r') as f:
+                script = f.read()
+        else:
+            # Fallback simple script
+            script = f'''
 import openai
 import os
 import json
@@ -349,8 +362,9 @@ response = openai.ChatCompletion.create(
 print(response.choices[0].message.content)
 '''
         
-        # Write and execute the script
+        # Write the script and prompt to sandbox
         await sandbox.filesystem.write("/tmp/codex_agent.py", script)
+        await sandbox.filesystem.write("/tmp/agent_prompt.txt", prompt)
         
         try:
             # Check if OpenAI is already installed (in custom template)
@@ -428,49 +442,3 @@ def run_ai_code_task_e2b(task_id: int, user_id: str, prompt: str,
         raise
 
 
-def parse_file_changes(git_diff: str) -> List[Dict[str, Any]]:
-    """Parse git diff to extract individual file changes"""
-    file_changes = []
-    current_file = None
-    before_lines = []
-    after_lines = []
-    in_diff = False
-    
-    for line in git_diff.split('\n'):
-        if line.startswith('diff --git'):
-            # Save previous file if exists
-            if current_file:
-                file_changes.append({
-                    "path": current_file,
-                    "before": '\n'.join(before_lines),
-                    "after": '\n'.join(after_lines)
-                })
-            
-            # Extract filename
-            parts = line.split(' ')
-            if len(parts) >= 4:
-                current_file = parts[3][2:] if parts[3].startswith('b/') else parts[3]
-            before_lines = []
-            after_lines = []
-            in_diff = False
-            
-        elif line.startswith('@@'):
-            in_diff = True
-        elif in_diff and current_file:
-            if line.startswith('-') and not line.startswith('---'):
-                before_lines.append(line[1:])
-            elif line.startswith('+') and not line.startswith('+++'):
-                after_lines.append(line[1:])
-            elif not line.startswith('\\'):
-                before_lines.append(line[1:] if line else '')
-                after_lines.append(line[1:] if line else '')
-    
-    # Save last file
-    if current_file:
-        file_changes.append({
-            "path": current_file,
-            "before": '\n'.join(before_lines),
-            "after": '\n'.join(after_lines)
-        })
-    
-    return file_changes
\ No newline at end of file
diff --git a/server-e2b/utils/code_task_v1.py b/server-e2b/utils/code_task_v1.py
deleted file mode 100644
index 4a63591..0000000
--- a/server-e2b/utils/code_task_v1.py
+++ /dev/null
@@ -1,353 +0,0 @@
-import json
-import os
-import logging
-import docker
-import docker.types
-import uuid
-import time
-from models import TaskStatus
-
-from .container import cleanup_orphaned_containers
-import sys
-sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
-from config import get_container_user_mapping, get_workspace_path, get_security_options, CONTAINER_UID, CONTAINER_GID
-from utils.validators import TaskInputValidator
-from utils.secure_exec import create_safe_docker_script
-
-# Configure logging
-logging.basicConfig(level=logging.INFO)
-logger = logging.getLogger(__name__)
-
-# Docker client
-docker_client = docker.from_env()
-
-# Legacy in-memory task storage (for backward compatibility)
-tasks = {}
-
-# Simple persistence for tasks (save to file)
-TASKS_FILE = 'tasks_backup.json'
-
-def save_tasks():
-    """Save tasks to file for persistence"""
-    try:
-        with open(TASKS_FILE, 'w') as f:
-            json.dump(tasks, f, indent=2, default=str)
-        logger.info(f"üíæ Saved {len(tasks)} tasks to {TASKS_FILE}")
-    except Exception as e:
-        logger.warning(f"‚ö†Ô∏è Failed to save tasks: {e}")
-
-def load_tasks():
-    """Load tasks from file"""
-    global tasks
-    try:
-        if os.path.exists(TASKS_FILE):
-            with open(TASKS_FILE, 'r') as f:
-                tasks = json.load(f)
-            logger.info(f"üìÇ Loaded {len(tasks)} tasks from {TASKS_FILE}")
-        else:
-            logger.info(f"üìÇ No tasks file found, starting fresh")
-    except Exception as e:
-        logger.warning(f"‚ö†Ô∏è Failed to load tasks: {e}")
-        tasks = {}
-
-
-# Load tasks on startup
-load_tasks()
-
-# Legacy function for backward compatibility
-def run_ai_code_task(task_id):
-    """Legacy function - should not be used with new Supabase system"""
-    logger.warning(f"Legacy run_ai_code_task called for task {task_id} - this should be migrated to use run_ai_code_task_v2")
-    
-    try:
-        # Check if task exists and get model type
-        if task_id not in tasks:
-            logger.error(f"Task {task_id} not found in tasks")
-            return
-            
-        task = tasks[task_id]
-        model_cli = task.get('model', 'claude')
-        
-        # With comprehensive sandboxing fixes, both Claude and Codex can now run in parallel
-        logger.info(f"üöÄ Running legacy {model_cli.upper()} task {task_id} directly in parallel mode")
-        return _run_ai_code_task_internal(task_id)
-            
-    except Exception as e:
-        logger.error(f"üí• Exception in run_ai_code_task: {str(e)}")
-        if task_id in tasks:
-            tasks[task_id]['status'] = TaskStatus.FAILED
-            tasks[task_id]['error'] = str(e)
-
-def _run_ai_code_task_internal(task_id):
-    """Internal implementation of legacy AI Code automation - called directly for Claude or via queue for Codex"""
-    try:
-        task = tasks[task_id]
-        task['status'] = TaskStatus.RUNNING
-        
-        model_name = task.get('model', 'claude').upper()
-        logger.info(f"üöÄ Starting {model_name} Code task {task_id}")
-        
-        # Validate inputs using Pydantic model
-        try:
-            validated_inputs = TaskInputValidator(
-                task_id=str(task_id),
-                repo_url=task['repo_url'],
-                target_branch=task['branch'],
-                prompt=task['prompt'],
-                model=task.get('model', 'claude'),
-                github_username=task.get('github_username')
-            )
-        except Exception as validation_error:
-            error_msg = f"Input validation failed: {str(validation_error)}"
-            logger.error(error_msg)
-            task['status'] = TaskStatus.FAILED
-            task['error'] = error_msg
-            save_tasks()
-            return
-        
-        logger.info(f"üìã Task details: prompt='{validated_inputs.prompt[:50]}...', repo={validated_inputs.repo_url}, branch={validated_inputs.target_branch}, model={model_name}")
-        logger.info(f"Starting {model_name} task {task_id}")
-        
-        # Create container environment variables
-        env_vars = {
-            'CI': 'true',  # Indicate we're in CI/non-interactive environment
-            'TERM': 'dumb',  # Use dumb terminal to avoid interactive features
-            'NO_COLOR': '1',  # Disable colors for cleaner output
-            'FORCE_COLOR': '0',  # Disable colors for cleaner output
-            'NONINTERACTIVE': '1',  # Common flag for non-interactive mode
-            'DEBIAN_FRONTEND': 'noninteractive',  # Non-interactive package installs
-        }
-        
-        # Add model-specific API keys and environment variables
-        model_cli = validated_inputs.model
-        if model_cli == 'claude':
-            env_vars.update({
-                'ANTHROPIC_API_KEY': os.getenv('ANTHROPIC_API_KEY'),
-                'ANTHROPIC_NONINTERACTIVE': '1'  # Custom flag for Anthropic tools
-            })
-        elif model_cli == 'codex':
-            env_vars.update({
-                'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),
-                'OPENAI_NONINTERACTIVE': '1',  # Custom flag for OpenAI tools
-                'CODEX_QUIET_MODE': '1'  # Official Codex non-interactive flag
-            })
-        
-        # Use specialized container images based on model
-        if model_cli == 'codex':
-            container_image = 'codex-automation:latest'
-        else:
-            container_image = 'claude-code-automation:latest'
-        
-        # Ensure workspace permissions for non-root container execution
-        workspace_path = get_workspace_path(task_id)
-        try:
-            os.makedirs(workspace_path, exist_ok=True)
-            # Set ownership to configured UID/GID for container user
-            os.chown(workspace_path, CONTAINER_UID, CONTAINER_GID)
-            logger.info(f"üîß Created workspace with proper permissions: {workspace_path} (UID:{CONTAINER_UID}, GID:{CONTAINER_GID})")
-        except Exception as e:
-            logger.warning(f"‚ö†Ô∏è  Could not set workspace permissions: {e}")
-        
-        # Create the command to run in container using secure method
-        container_command = create_safe_docker_script(
-            repo_url=validated_inputs.repo_url,
-            branch=validated_inputs.target_branch,
-            prompt=validated_inputs.prompt,
-            model_cli=validated_inputs.model,
-            github_username=validated_inputs.github_username
-        )
-        
-        # Run container with unified AI Code tools (supports both Claude and Codex)
-        logger.info(f"üê≥ Creating Docker container for task {task_id} using {container_image} (model: {model_name})")
-        
-        # Configure Docker security options for Codex compatibility
-        container_kwargs = {
-            'image': container_image,
-            'command': ['bash', '-c', container_command],
-            'environment': env_vars,
-            'detach': True,
-            'remove': False,  # Don't auto-remove so we can get logs
-            'working_dir': '/workspace',
-            'network_mode': 'bridge',  # Ensure proper networking
-            'tty': False,  # Don't allocate TTY - may prevent clean exit
-            'stdin_open': False,  # Don't keep stdin open - may prevent clean exit
-            'name': f'ai-code-task-{task_id}-{int(time.time())}-{uuid.uuid4().hex[:8]}',  # Highly unique container name with UUID
-            'mem_limit': '2g',  # Limit memory usage to prevent resource conflicts
-            'cpu_shares': 1024,  # Standard CPU allocation
-            'ulimits': [docker.types.Ulimit(name='nofile', soft=1024, hard=2048)],  # File descriptor limits
-            'volumes': {
-                workspace_path: {'bind': '/workspace/tmp', 'mode': 'rw'}  # Mount workspace with proper permissions
-            }
-        }
-        
-        # Add security configurations for better isolation
-        logger.info(f"üîí Running {model_name} with secure container configuration")
-        container_kwargs.update({
-            # Security options for better isolation
-            'security_opt': get_security_options(),
-            'read_only': False,            # Allow writes to workspace only
-            'user': get_container_user_mapping()  # Run as configured non-root user
-        })
-        
-        # Retry container creation with enhanced conflict handling
-        container = None
-        max_retries = 5  # Increased retries for better reliability
-        for attempt in range(max_retries):
-            try:
-                logger.info(f"üîÑ Container creation attempt {attempt + 1}/{max_retries}")
-                container = docker_client.containers.run(**container_kwargs)
-                logger.info(f"‚úÖ Container created successfully: {container.id[:12]} (name: {container_kwargs['name']})")
-                break
-            except docker.errors.APIError as e:
-                error_msg = str(e)
-                if "Conflict" in error_msg and "already in use" in error_msg:
-                    # Handle container name conflicts by generating a new unique name
-                    logger.warning(f"üîÑ Container name conflict on attempt {attempt + 1}, generating new name...")
-                    new_name = f'ai-code-task-{task_id}-{int(time.time())}-{uuid.uuid4().hex[:8]}'
-                    container_kwargs['name'] = new_name
-                    logger.info(f"üÜî New container name: {new_name}")
-                    # Try to clean up any conflicting containers
-                    cleanup_orphaned_containers()
-                else:
-                    logger.warning(f"‚ö†Ô∏è  Docker API error on attempt {attempt + 1}: {e}")
-                    if attempt == max_retries - 1:
-                        raise Exception(f"Failed to create container after {max_retries} attempts: {e}")
-                time.sleep(2 ** attempt)  # Exponential backoff
-            except Exception as e:
-                logger.error(f"‚ùå Unexpected error creating container on attempt {attempt + 1}: {e}")
-                if attempt == max_retries - 1:
-                    raise
-                time.sleep(2 ** attempt)  # Exponential backoff
-        
-        task['container_id'] = container.id  # Legacy function
-        logger.info(f"‚è≥ Waiting for container to complete (timeout: 300s)...")
-        
-        # Wait for container to finish - should exit naturally when script completes
-        try:
-            logger.info(f"üîÑ Waiting for container script to complete naturally...")
-            
-            # Check initial container state
-            container.reload()
-            logger.info(f"üîç Container initial state: {container.status}")
-            
-            # Use standard wait - container should exit when bash script finishes
-            logger.info(f"üîÑ Calling container.wait() - container should exit when script completes...")
-            result = container.wait(timeout=300)  # 5 minute timeout
-            logger.info(f"üéØ Container exited naturally! Exit code: {result['StatusCode']}")
-            
-            # Verify final container state
-            container.reload()
-            logger.info(f"üîç Final container state: {container.status}")
-            
-            # Get logs before any cleanup operations
-            logger.info(f"üìú Retrieving container logs...")
-            try:
-                logs = container.logs().decode('utf-8')
-                logger.info(f"üìù Retrieved {len(logs)} characters of logs")
-                logger.info(f"üîç First 200 chars of logs: {logs[:200]}...")
-            except Exception as log_error:
-                logger.warning(f"‚ùå Failed to get container logs: {log_error}")
-                logs = f"Failed to retrieve logs: {log_error}"
-            
-            # Clean up container after getting logs
-            try:
-                container.reload()  # Refresh container state
-                container.remove()
-                logger.info(f"Successfully removed container {container.id}")
-            except Exception as cleanup_error:
-                logger.warning(f"Failed to remove container {container.id}: {cleanup_error}")
-                # Try force removal as fallback
-                try:
-                    container.remove(force=True)
-                    logger.info(f"Force removed container {container.id}")
-                except Exception as force_cleanup_error:
-                    logger.error(f"Failed to force remove container: {force_cleanup_error}")
-                
-        except Exception as e:
-            logger.error(f"‚è∞ Container timeout or error: {str(e)}")
-            logger.error(f"üîÑ Updating task status to FAILED due to timeout/error...")
-            task['status'] = TaskStatus.FAILED
-            task['error'] = f"Container execution timeout or error: {str(e)}"
-            
-            # Try to get logs even on error
-            try:
-                logs = container.logs().decode('utf-8')
-            except Exception as log_error:
-                logs = f"Container failed and logs unavailable: {log_error}"
-            
-            # Try to clean up container on error
-            try:
-                container.reload()  # Refresh container state
-                container.remove(force=True)
-                logger.info(f"Cleaned up failed container {container.id}")
-            except Exception as cleanup_error:
-                logger.warning(f"Failed to remove failed container {container.id}: {cleanup_error}")
-            return
-        
-        if result['StatusCode'] == 0:
-            logger.info(f"‚úÖ Container exited successfully (code 0) - parsing results...")
-            # Parse output to extract commit hash, diff, and patch
-            lines = logs.split('\n')
-            commit_hash = None
-            git_diff = []
-            git_patch = []
-            changed_files = []
-            capturing_diff = False
-            capturing_patch = False
-            capturing_files = False
-            
-            for line in lines:
-                if line.startswith('COMMIT_HASH='):
-                    commit_hash = line.split('=', 1)[1]
-                    logger.info(f"üîë Found commit hash: {commit_hash}")
-                elif line == '=== PATCH START ===':
-                    capturing_patch = True
-                    logger.info(f"üì¶ Starting to capture git patch...")
-                elif line == '=== PATCH END ===':
-                    capturing_patch = False
-                    logger.info(f"üì¶ Finished capturing git patch ({len(git_patch)} lines)")
-                elif line == '=== GIT DIFF START ===':
-                    capturing_diff = True
-                    logger.info(f"üìä Starting to capture git diff...")
-                elif line == '=== GIT DIFF END ===':
-                    capturing_diff = False
-                    logger.info(f"üìä Finished capturing git diff ({len(git_diff)} lines)")
-                elif line == '=== CHANGED FILES START ===':
-                    capturing_files = True
-                    logger.info(f"üìÅ Starting to capture changed files...")
-                elif line == '=== CHANGED FILES END ===':
-                    capturing_files = False
-                    logger.info(f"üìÅ Finished capturing changed files ({len(changed_files)} files)")
-                elif capturing_patch:
-                    git_patch.append(line)
-                elif capturing_diff:
-                    git_diff.append(line)
-                elif capturing_files:
-                    if line.strip():  # Only add non-empty lines
-                        changed_files.append(line.strip())
-            
-            logger.info(f"üîÑ Updating task status to COMPLETED...")
-            task['status'] = TaskStatus.COMPLETED
-            task['commit_hash'] = commit_hash
-            task['git_diff'] = '\n'.join(git_diff)
-            task['git_patch'] = '\n'.join(git_patch)
-            task['changed_files'] = changed_files
-            
-            # Save tasks after completion
-            save_tasks()
-            
-            logger.info(f"üéâ {model_name} Task {task_id} completed successfully! Commit: {commit_hash[:8] if commit_hash else 'N/A'}, Diff lines: {len(git_diff)}")
-            
-        else:
-            logger.error(f"‚ùå Container exited with error code {result['StatusCode']}")
-            task['status'] = TaskStatus.FAILED
-            task['error'] = f"Container exited with code {result['StatusCode']}: {logs}"
-            save_tasks()  # Save failed task
-            logger.error(f"üí• {model_name} Task {task_id} failed: {task['error'][:200]}...")
-            
-    except Exception as e:
-        model_name = task.get('model', 'claude').upper()
-        logger.error(f"üí• Unexpected exception in {model_name} task {task_id}: {str(e)}")
-        task['status'] = TaskStatus.FAILED
-        task['error'] = str(e)
-        logger.error(f"üîÑ {model_name} Task {task_id} failed with exception: {str(e)}")
diff --git a/server-e2b/utils/code_task_v2.py b/server-e2b/utils/code_task_v2.py
deleted file mode 100644
index c04e07a..0000000
--- a/server-e2b/utils/code_task_v2.py
+++ /dev/null
@@ -1,492 +0,0 @@
-import json
-import os
-import logging
-import docker
-import docker.types
-import uuid
-import time
-import random
-from datetime import datetime
-from database import DatabaseOperations
-import fcntl
-import sys
-sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
-from config import get_container_user_mapping, get_workspace_path, get_security_options, CONTAINER_UID, CONTAINER_GID
-from utils.validators import TaskInputValidator
-from utils.secure_exec import create_safe_docker_script
-
-# Configure logging
-logging.basicConfig(level=logging.INFO)
-logger = logging.getLogger(__name__)
-
-# Docker client
-docker_client = docker.from_env()
-
-def cleanup_orphaned_containers():
-    """Clean up orphaned AI code task containers aggressively"""
-    try:
-        # Get all containers with our naming pattern
-        containers = docker_client.containers.list(all=True, filters={'name': 'ai-code-task-'})
-        orphaned_count = 0
-        current_time = time.time()
-        
-        for container in containers:
-            try:
-                # Get container creation time
-                created_at = container.attrs['Created']
-                # Parse ISO format timestamp and convert to epoch time
-                created_time = datetime.fromisoformat(created_at.replace('Z', '+00:00')).timestamp()
-                age_hours = (current_time - created_time) / 3600
-                
-                # Remove containers that are:
-                # 1. Not running (exited, dead, created)
-                # 2. OR older than 2 hours (stuck containers)
-                # 3. OR in error state
-                should_remove = (
-                    container.status in ['exited', 'dead', 'created'] or
-                    age_hours > 2 or
-                    container.status == 'restarting'
-                )
-                
-                if should_remove:
-                    logger.info(f"üßπ Removing orphaned container {container.id[:12]} (status: {container.status}, age: {age_hours:.1f}h)")
-                    container.remove(force=True)
-                    orphaned_count += 1
-                
-            except Exception as e:
-                logger.warning(f"‚ö†Ô∏è  Failed to cleanup container {container.id[:12]}: {e}")
-                # If we can't inspect it, try to force remove it anyway
-                try:
-                    container.remove(force=True)
-                    orphaned_count += 1
-                    logger.info(f"üßπ Force removed problematic container: {container.id[:12]}")
-                except Exception as force_error:
-                    logger.warning(f"‚ö†Ô∏è  Could not force remove container {container.id[:12]}: {force_error}")
-        
-        if orphaned_count > 0:
-            logger.info(f"üßπ Cleaned up {orphaned_count} orphaned containers")
-        
-    except Exception as e:
-        logger.warning(f"‚ö†Ô∏è  Failed to cleanup orphaned containers: {e}")
-
-def run_ai_code_task_v2(task_id: int, user_id: str, github_token: str):
-    """Run AI Code automation (Claude or Codex) in a container - Supabase version"""
-    try:
-        # Get task from database to check the model type
-        task = DatabaseOperations.get_task_by_id(task_id, user_id)
-        if not task:
-            logger.error(f"Task {task_id} not found in database")
-            return
-        
-        model_cli = task.get('agent', 'claude')
-        
-        # With comprehensive sandboxing fixes, both Claude and Codex can now run in parallel
-        logger.info(f"üöÄ Running {model_cli.upper()} task {task_id} directly in parallel mode")
-        return _run_ai_code_task_v2_internal(task_id, user_id, github_token)
-            
-    except Exception as e:
-        logger.error(f"üí• Exception in run_ai_code_task_v2: {str(e)}")
-        try:
-            DatabaseOperations.update_task(task_id, user_id, {
-                'status': 'failed',
-                'error': str(e)
-            })
-        except:
-            logger.error(f"Failed to update task {task_id} status after exception")
-
-def _run_ai_code_task_v2_internal(task_id: int, user_id: str, github_token: str):
-    """Internal implementation of AI Code automation - called directly for Claude or via queue for Codex"""
-    try:
-        # Clean up any orphaned containers before starting new task
-        cleanup_orphaned_containers()
-        
-        # Get task from database (v2 function)
-        task = DatabaseOperations.get_task_by_id(task_id, user_id)
-        if not task:
-            logger.error(f"Task {task_id} not found in database")
-            return
-        
-        # Update task status to running
-        DatabaseOperations.update_task(task_id, user_id, {'status': 'running'})
-        
-        model_name = task.get('agent', 'claude').upper()
-        logger.info(f"üöÄ Starting {model_name} Code task {task_id}")
-        
-        # Get prompt from chat messages
-        prompt = ""
-        if task.get('chat_messages'):
-            for msg in task['chat_messages']:
-                if msg.get('role') == 'user':
-                    prompt = msg.get('content', '')
-                    break
-        
-        if not prompt:
-            error_msg = "No user prompt found in chat messages"
-            logger.error(error_msg)
-            DatabaseOperations.update_task(task_id, user_id, {
-                'status': 'failed',
-                'error': error_msg
-            })
-            return
-        
-        # Validate inputs using Pydantic model
-        try:
-            validated_inputs = TaskInputValidator(
-                task_id=str(task_id),
-                repo_url=task['repo_url'],
-                target_branch=task['target_branch'],
-                prompt=prompt,
-                model=task.get('agent', 'claude'),
-                github_username=task.get('github_username')
-            )
-        except Exception as validation_error:
-            error_msg = f"Input validation failed: {str(validation_error)}"
-            logger.error(error_msg)
-            DatabaseOperations.update_task(task_id, user_id, {
-                'status': 'failed',
-                'error': error_msg
-            })
-            return
-        
-        logger.info(f"üìã Task details: prompt='{validated_inputs.prompt[:50]}...', repo={validated_inputs.repo_url}, branch={validated_inputs.target_branch}, model={model_name}")
-        logger.info(f"Starting {model_name} task {task_id}")
-        
-        # Create container environment variables
-        env_vars = {
-            'CI': 'true',  # Indicate we're in CI/non-interactive environment
-            'TERM': 'dumb',  # Use dumb terminal to avoid interactive features
-            'NO_COLOR': '1',  # Disable colors for cleaner output
-            'FORCE_COLOR': '0',  # Disable colors for cleaner output
-            'NONINTERACTIVE': '1',  # Common flag for non-interactive mode
-            'DEBIAN_FRONTEND': 'noninteractive',  # Non-interactive package installs
-        }
-        
-        # Add model-specific API keys and environment variables
-        model_cli = validated_inputs.model
-        if model_cli == 'claude':
-            env_vars.update({
-                'ANTHROPIC_API_KEY': os.getenv('ANTHROPIC_API_KEY'),
-                'ANTHROPIC_NONINTERACTIVE': '1'  # Custom flag for Anthropic tools
-            })
-        elif model_cli == 'codex':
-            env_vars.update({
-                'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),
-                'OPENAI_NONINTERACTIVE': '1',  # Custom flag for OpenAI tools
-                'CODEX_QUIET_MODE': '1'  # Official Codex non-interactive flag
-            })
-        
-        # Use specialized container images based on model
-        if model_cli == 'codex':
-            container_image = 'codex-automation:latest'
-        else:
-            container_image = 'claude-code-automation:latest'
-        
-        # Ensure workspace permissions for non-root container execution
-        workspace_path = get_workspace_path(task_id)
-        try:
-            os.makedirs(workspace_path, exist_ok=True)
-            # Set ownership to configured UID/GID for container user
-            os.chown(workspace_path, CONTAINER_UID, CONTAINER_GID)
-            logger.info(f"üîß Created workspace with proper permissions: {workspace_path} (UID:{CONTAINER_UID}, GID:{CONTAINER_GID})")
-        except Exception as e:
-            logger.warning(f"‚ö†Ô∏è  Could not set workspace permissions: {e}")
-        
-        # Add staggered start to prevent race conditions with parallel Codex tasks
-        if model_cli == 'codex':
-            # Random delay between 0.5-2 seconds for Codex containers to prevent resource conflicts
-            stagger_delay = random.uniform(0.5, 2.0)
-            logger.info(f"üïê Adding {stagger_delay:.1f}s staggered start delay for Codex task {task_id}")
-            time.sleep(stagger_delay)
-            
-            # Add file-based locking for Codex to prevent parallel execution conflicts
-            lock_file_path = '/tmp/codex_execution_lock'
-            try:
-                logger.info(f"üîí Acquiring Codex execution lock for task {task_id}")
-                with open(lock_file_path, 'w') as lock_file:
-                    fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
-                    logger.info(f"‚úÖ Codex execution lock acquired for task {task_id}")
-                    # Continue with container creation while holding the lock
-            except (IOError, OSError) as e:
-                logger.warning(f"‚ö†Ô∏è  Could not acquire Codex execution lock for task {task_id}: {e}")
-                # Add additional delay if lock fails
-                additional_delay = random.uniform(1.0, 3.0)
-                logger.info(f"üïê Adding additional {additional_delay:.1f}s delay due to lock conflict")
-                time.sleep(additional_delay)
-        
-        # Create the command to run in container using secure method
-        container_command = create_safe_docker_script(
-            repo_url=validated_inputs.repo_url,
-            branch=validated_inputs.target_branch,
-            prompt=validated_inputs.prompt,
-            model_cli=validated_inputs.model,
-            github_username=validated_inputs.github_username
-        )
-        
-        # Run container with unified AI Code tools (supports both Claude and Codex)
-        logger.info(f"üê≥ Creating Docker container for task {task_id} using {container_image} (model: {model_name})")
-        
-        # Configure Docker security options for Codex compatibility
-        container_kwargs = {
-            'image': container_image,
-            'command': ['bash', '-c', container_command],
-            'environment': env_vars,
-            'detach': True,
-            'remove': False,  # Don't auto-remove so we can get logs
-            'working_dir': '/workspace',
-            'network_mode': 'bridge',  # Ensure proper networking
-            'tty': False,  # Don't allocate TTY - may prevent clean exit
-            'stdin_open': False,  # Don't keep stdin open - may prevent clean exit
-            'name': f'ai-code-task-{task_id}-{int(time.time())}-{uuid.uuid4().hex[:8]}',  # Highly unique container name with UUID
-            'mem_limit': '2g',  # Limit memory usage to prevent resource conflicts
-            'cpu_shares': 1024,  # Standard CPU allocation
-            'ulimits': [docker.types.Ulimit(name='nofile', soft=1024, hard=2048)],  # File descriptor limits
-            'volumes': {
-                workspace_path: {'bind': '/workspace/tmp', 'mode': 'rw'}  # Mount workspace with proper permissions
-            }
-        }
-        
-        # Add security configurations for better isolation
-        logger.info(f"üîí Running {model_name} with secure container configuration")
-        container_kwargs.update({
-            # Security options for better isolation
-            'security_opt': get_security_options(),
-            'read_only': False,            # Allow writes to workspace only
-            'user': get_container_user_mapping()  # Run as configured non-root user
-        })
-        
-        # Retry container creation with enhanced conflict handling
-        container = None
-        max_retries = 5  # Increased retries for better reliability
-        for attempt in range(max_retries):
-            try:
-                logger.info(f"üîÑ Container creation attempt {attempt + 1}/{max_retries}")
-                container = docker_client.containers.run(**container_kwargs)
-                logger.info(f"‚úÖ Container created successfully: {container.id[:12]} (name: {container_kwargs['name']})")
-                break
-            except docker.errors.APIError as e:
-                error_msg = str(e)
-                if "Conflict" in error_msg and "already in use" in error_msg:
-                    # Handle container name conflicts by generating a new unique name
-                    logger.warning(f"üîÑ Container name conflict on attempt {attempt + 1}, generating new name...")
-                    new_name = f'ai-code-task-{task_id}-{int(time.time())}-{uuid.uuid4().hex[:8]}'
-                    container_kwargs['name'] = new_name
-                    logger.info(f"üÜî New container name: {new_name}")
-                    # Try to clean up any conflicting containers
-                    cleanup_orphaned_containers()
-                else:
-                    logger.warning(f"‚ö†Ô∏è  Docker API error on attempt {attempt + 1}: {e}")
-                    if attempt == max_retries - 1:
-                        raise Exception(f"Failed to create container after {max_retries} attempts: {e}")
-                time.sleep(2 ** attempt)  # Exponential backoff
-            except Exception as e:
-                logger.error(f"‚ùå Unexpected error creating container on attempt {attempt + 1}: {e}")
-                if attempt == max_retries - 1:
-                    raise
-                time.sleep(2 ** attempt)  # Exponential backoff
-        
-        # Update task with container ID (v2 function)
-        DatabaseOperations.update_task(task_id, user_id, {'container_id': container.id})
-        
-        logger.info(f"‚è≥ Waiting for container to complete (timeout: 300s)...")
-        
-        # Wait for container to finish - should exit naturally when script completes
-        try:
-            logger.info(f"üîÑ Waiting for container script to complete naturally...")
-            
-            # Check initial container state
-            container.reload()
-            logger.info(f"üîç Container initial state: {container.status}")
-            
-            # Use standard wait - container should exit when bash script finishes
-            logger.info(f"üîÑ Calling container.wait() - container should exit when script completes...")
-            result = container.wait(timeout=300)  # 5 minute timeout
-            logger.info(f"üéØ Container exited naturally! Exit code: {result['StatusCode']}")
-            
-            # Verify final container state
-            container.reload()
-            logger.info(f"üîç Final container state: {container.status}")
-            
-            # Get logs before any cleanup operations
-            logger.info(f"üìú Retrieving container logs...")
-            try:
-                logs = container.logs().decode('utf-8')
-                logger.info(f"üìù Retrieved {len(logs)} characters of logs")
-                logger.info(f"üîç First 200 chars of logs: {logs[:200]}...")
-            except Exception as log_error:
-                logger.warning(f"‚ùå Failed to get container logs: {log_error}")
-                logs = f"Failed to retrieve logs: {log_error}"
-            
-            # Clean up container after getting logs
-            try:
-                container.reload()  # Refresh container state
-                container.remove()
-                logger.info(f"üßπ Successfully removed container {container.id[:12]}")
-            except docker.errors.NotFound:
-                logger.info(f"üßπ Container {container.id[:12]} already removed")
-            except Exception as cleanup_error:
-                logger.warning(f"‚ö†Ô∏è  Failed to remove container {container.id[:12]}: {cleanup_error}")
-                # Try force removal as fallback
-                try:
-                    container.remove(force=True)
-                    logger.info(f"üßπ Force removed container {container.id[:12]}")
-                except docker.errors.NotFound:
-                    logger.info(f"üßπ Container {container.id[:12]} already removed")
-                except Exception as force_cleanup_error:
-                    logger.error(f"‚ùå Failed to force remove container {container.id[:12]}: {force_cleanup_error}")
-                
-        except Exception as e:
-            logger.error(f"‚è∞ Container timeout or error: {str(e)}")
-            logger.error(f"üîÑ Updating task status to FAILED due to timeout/error...")
-            
-            DatabaseOperations.update_task(task_id, user_id, {
-                'status': 'failed',
-                'error': f"Container execution timeout or error: {str(e)}"
-            })
-            
-            # Try to get logs even on error
-            try:
-                logs = container.logs().decode('utf-8')
-            except Exception as log_error:
-                logs = f"Container failed and logs unavailable: {log_error}"
-            
-            # Try to clean up container on error
-            try:
-                container.reload()  # Refresh container state
-                container.remove(force=True)
-                logger.info(f"Cleaned up failed container {container.id}")
-            except Exception as cleanup_error:
-                logger.warning(f"Failed to remove failed container {container.id}: {cleanup_error}")
-            return
-        
-        if result['StatusCode'] == 0:
-            logger.info(f"‚úÖ Container exited successfully (code 0) - parsing results...")
-            # Parse output to extract commit hash, diff, and patch
-            lines = logs.split('\n')
-            commit_hash = None
-            git_diff = []
-            git_patch = []
-            changed_files = []
-            file_changes = []
-            capturing_diff = False
-            capturing_patch = False
-            capturing_files = False
-            capturing_file_changes = False
-            capturing_before = False
-            capturing_after = False
-            current_file = None
-            current_before = []
-            current_after = []
-            
-            for line in lines:
-                if line.startswith('COMMIT_HASH='):
-                    commit_hash = line.split('=', 1)[1]
-                    logger.info(f"üîë Found commit hash: {commit_hash}")
-                elif line == '=== PATCH START ===':
-                    capturing_patch = True
-                    logger.info(f"üì¶ Starting to capture git patch...")
-                elif line == '=== PATCH END ===':
-                    capturing_patch = False
-                    logger.info(f"üì¶ Finished capturing git patch ({len(git_patch)} lines)")
-                elif line == '=== GIT DIFF START ===':
-                    capturing_diff = True
-                    logger.info(f"üìä Starting to capture git diff...")
-                elif line == '=== GIT DIFF END ===':
-                    capturing_diff = False
-                    logger.info(f"üìä Finished capturing git diff ({len(git_diff)} lines)")
-                elif line == '=== CHANGED FILES START ===':
-                    capturing_files = True
-                    logger.info(f"üìÅ Starting to capture changed files...")
-                elif line == '=== CHANGED FILES END ===':
-                    capturing_files = False
-                    logger.info(f"üìÅ Finished capturing changed files ({len(changed_files)} files)")
-                elif line == '=== FILE CHANGES START ===':
-                    capturing_file_changes = True
-                    logger.info(f"üîÑ Starting to capture file changes...")
-                elif line == '=== FILE CHANGES END ===':
-                    capturing_file_changes = False
-                    # Add the last file if we were processing one
-                    if current_file:
-                        file_changes.append({
-                            'filename': current_file,
-                            'before': '\n'.join(current_before),
-                            'after': '\n'.join(current_after)
-                        })
-                    logger.info(f"üîÑ Finished capturing file changes ({len(file_changes)} files)")
-                elif capturing_file_changes:
-                    if line.startswith('FILE: '):
-                        # Save previous file data if exists
-                        if current_file:
-                            file_changes.append({
-                                'filename': current_file,
-                                'before': '\n'.join(current_before),
-                                'after': '\n'.join(current_after)
-                            })
-                        # Start new file
-                        current_file = line.split('FILE: ', 1)[1]
-                        current_before = []
-                        current_after = []
-                        capturing_before = False
-                        capturing_after = False
-                    elif line == '=== BEFORE START ===':
-                        capturing_before = True
-                        capturing_after = False
-                    elif line == '=== BEFORE END ===':
-                        capturing_before = False
-                    elif line == '=== AFTER START ===':
-                        capturing_after = True
-                        capturing_before = False
-                    elif line == '=== AFTER END ===':
-                        capturing_after = False
-                    elif line == '=== FILE END ===':
-                        # File processing complete
-                        pass
-                    elif capturing_before:
-                        current_before.append(line)
-                    elif capturing_after:
-                        current_after.append(line)
-                elif capturing_patch:
-                    git_patch.append(line)
-                elif capturing_diff:
-                    git_diff.append(line)
-                elif capturing_files:
-                    if line.strip():  # Only add non-empty lines
-                        changed_files.append(line.strip())
-            
-            logger.info(f"üîÑ Updating task status to COMPLETED...")
-            
-            # Update task in database
-            DatabaseOperations.update_task(task_id, user_id, {
-                'status': 'completed',
-                'commit_hash': commit_hash,
-                'git_diff': '\n'.join(git_diff),
-                'git_patch': '\n'.join(git_patch),
-                'changed_files': changed_files,
-                'execution_metadata': {
-                    'file_changes': file_changes,
-                    'completed_at': datetime.now().isoformat()
-                }
-            })
-            
-            logger.info(f"üéâ {model_name} Task {task_id} completed successfully! Commit: {commit_hash[:8] if commit_hash else 'N/A'}, Diff lines: {len(git_diff)}")
-            
-        else:
-            logger.error(f"‚ùå Container exited with error code {result['StatusCode']}")
-            DatabaseOperations.update_task(task_id, user_id, {
-                'status': 'failed',
-                'error': f"Container exited with code {result['StatusCode']}: {logs}"
-            })
-            logger.error(f"üí• {model_name} Task {task_id} failed: {logs[:200]}...")
-            
-    except Exception as e:
-        model_name = task.get('agent', 'claude').upper() if task else 'UNKNOWN'
-        logger.error(f"üí• Unexpected exception in {model_name} task {task_id}: {str(e)}")
-        
-        try:
-            DatabaseOperations.update_task(task_id, user_id, {
-                'status': 'failed',
-                'error': str(e)
-            })
-        except:
-            logger.error(f"Failed to update task {task_id} status after exception")
-        
-        logger.error(f"üîÑ {model_name} Task {task_id} failed with exception: {str(e)}")
diff --git a/server-e2b/utils/container.py b/server-e2b/utils/container.py
deleted file mode 100644
index 19f2dc4..0000000
--- a/server-e2b/utils/container.py
+++ /dev/null
@@ -1,58 +0,0 @@
-import logging
-import docker
-import docker.types
-import time
-from datetime import datetime
-
-# Configure logging
-logging.basicConfig(level=logging.INFO)
-logger = logging.getLogger(__name__)
-# Docker client
-docker_client = docker.from_env()
-
-def cleanup_orphaned_containers():
-    """Clean up orphaned AI code task containers aggressively"""
-    try:
-        # Get all containers with our naming pattern
-        containers = docker_client.containers.list(all=True, filters={'name': 'ai-code-task-'})
-        orphaned_count = 0
-        current_time = time.time()
-        
-        for container in containers:
-            try:
-                # Get container creation time
-                created_at = container.attrs['Created']
-                # Parse ISO format timestamp and convert to epoch time
-                created_time = datetime.fromisoformat(created_at.replace('Z', '+00:00')).timestamp()
-                age_hours = (current_time - created_time) / 3600
-                
-                # Remove containers that are:
-                # 1. Not running (exited, dead, created)
-                # 2. OR older than 2 hours (stuck containers)
-                # 3. OR in error state
-                should_remove = (
-                    container.status in ['exited', 'dead', 'created'] or
-                    age_hours > 2 or
-                    container.status == 'restarting'
-                )
-                
-                if should_remove:
-                    logger.info(f"üßπ Removing orphaned container {container.id[:12]} (status: {container.status}, age: {age_hours:.1f}h)")
-                    container.remove(force=True)
-                    orphaned_count += 1
-                
-            except Exception as e:
-                logger.warning(f"‚ö†Ô∏è  Failed to cleanup container {container.id[:12]}: {e}")
-                # If we can't inspect it, try to force remove it anyway
-                try:
-                    container.remove(force=True)
-                    orphaned_count += 1
-                    logger.info(f"üßπ Force removed problematic container: {container.id[:12]}")
-                except Exception as force_error:
-                    logger.warning(f"‚ö†Ô∏è  Could not force remove container {container.id[:12]}: {force_error}")
-        
-        if orphaned_count > 0:
-            logger.info(f"üßπ Cleaned up {orphaned_count} orphaned containers")
-        
-    except Exception as e:
-        logger.warning(f"‚ö†Ô∏è  Failed to cleanup orphaned containers: {e}")
diff --git a/server-e2b/utils/git_utils.py b/server-e2b/utils/git_utils.py
new file mode 100644
index 0000000..11a3b73
--- /dev/null
+++ b/server-e2b/utils/git_utils.py
@@ -0,0 +1,60 @@
+"""
+Git-related utility functions.
+"""
+from typing import List, Dict, Any
+
+
+def parse_file_changes(git_diff: str) -> List[Dict[str, Any]]:
+    """
+    Parse git diff to extract individual file changes.
+    
+    Args:
+        git_diff: Raw git diff output
+        
+    Returns:
+        List of dicts with 'path', 'before', and 'after' content for each file
+    """
+    file_changes = []
+    current_file = None
+    before_lines = []
+    after_lines = []
+    in_diff = False
+    
+    for line in git_diff.split('\n'):
+        if line.startswith('diff --git'):
+            # Save previous file if exists
+            if current_file:
+                file_changes.append({
+                    "path": current_file,
+                    "before": '\n'.join(before_lines),
+                    "after": '\n'.join(after_lines)
+                })
+            
+            # Extract filename
+            parts = line.split(' ')
+            if len(parts) >= 4:
+                current_file = parts[3][2:] if parts[3].startswith('b/') else parts[3]
+            before_lines = []
+            after_lines = []
+            in_diff = False
+            
+        elif line.startswith('@@'):
+            in_diff = True
+        elif in_diff and current_file:
+            if line.startswith('-') and not line.startswith('---'):
+                before_lines.append(line[1:])
+            elif line.startswith('+') and not line.startswith('+++'):
+                after_lines.append(line[1:])
+            elif not line.startswith('\\'):
+                before_lines.append(line[1:] if line else '')
+                after_lines.append(line[1:] if line else '')
+    
+    # Save last file
+    if current_file:
+        file_changes.append({
+            "path": current_file,
+            "before": '\n'.join(before_lines),
+            "after": '\n'.join(after_lines)
+        })
+    
+    return file_changes
\ No newline at end of file
diff --git a/server-e2b/utils/secure_exec.py b/server-e2b/utils/secure_exec.py
deleted file mode 100644
index 3e89590..0000000
--- a/server-e2b/utils/secure_exec.py
+++ /dev/null
@@ -1,300 +0,0 @@
-"""Secure command execution utilities."""
-
-import shlex
-import subprocess
-from typing import List, Tuple, Optional
-import logging
-
-logger = logging.getLogger(__name__)
-
-
-def safe_git_clone(repo_url: str, branch: str, target_dir: str) -> Tuple[int, str, str]:
-    """
-    Safely clone a git repository using subprocess with shell=False.
-    
-    Args:
-        repo_url: The validated repository URL
-        branch: The validated branch name
-        target_dir: The target directory path
-        
-    Returns:
-        Tuple of (return_code, stdout, stderr)
-    """
-    # Build command as a list for shell=False
-    cmd = [
-        'git', 'clone',
-        '-b', branch,
-        repo_url,
-        target_dir
-    ]
-    
-    logger.info(f"Executing git clone: {' '.join(cmd)}")
-    
-    try:
-        result = subprocess.run(
-            cmd,
-            capture_output=True,
-            text=True,
-            timeout=300,  # 5 minute timeout
-            check=False
-        )
-        return result.returncode, result.stdout, result.stderr
-    except subprocess.TimeoutExpired:
-        logger.error("Git clone timed out after 5 minutes")
-        return 1, "", "Git clone operation timed out"
-    except Exception as e:
-        logger.error(f"Error executing git clone: {e}")
-        return 1, "", str(e)
-
-
-def safe_git_config(config_key: str, config_value: str, repo_dir: str) -> Tuple[int, str, str]:
-    """
-    Safely set git configuration using subprocess with shell=False.
-    
-    Args:
-        config_key: The git config key (e.g., 'user.email')
-        config_value: The config value
-        repo_dir: The repository directory
-        
-    Returns:
-        Tuple of (return_code, stdout, stderr)
-    """
-    # Build command as a list
-    cmd = ['git', 'config', config_key, config_value]
-    
-    try:
-        result = subprocess.run(
-            cmd,
-            cwd=repo_dir,
-            capture_output=True,
-            text=True,
-            timeout=30,
-            check=False
-        )
-        return result.returncode, result.stdout, result.stderr
-    except Exception as e:
-        logger.error(f"Error setting git config: {e}")
-        return 1, "", str(e)
-
-
-def safe_git_commit(message: str, repo_dir: str) -> Tuple[int, str, str]:
-    """
-    Safely create a git commit using subprocess with shell=False.
-    
-    Args:
-        message: The commit message (will be properly escaped)
-        repo_dir: The repository directory
-        
-    Returns:
-        Tuple of (return_code, stdout, stderr)
-    """
-    # Build command as a list - no need to escape when using shell=False
-    cmd = ['git', 'commit', '-m', message]
-    
-    try:
-        result = subprocess.run(
-            cmd,
-            cwd=repo_dir,
-            capture_output=True,
-            text=True,
-            timeout=60,
-            check=False
-        )
-        return result.returncode, result.stdout, result.stderr
-    except Exception as e:
-        logger.error(f"Error creating git commit: {e}")
-        return 1, "", str(e)
-
-
-def safe_git_command(git_args: List[str], repo_dir: str, timeout: int = 60) -> Tuple[int, str, str]:
-    """
-    Safely execute any git command using subprocess with shell=False.
-    
-    Args:
-        git_args: List of git command arguments (e.g., ['diff', 'HEAD~1', 'HEAD'])
-        repo_dir: The repository directory
-        timeout: Command timeout in seconds
-        
-    Returns:
-        Tuple of (return_code, stdout, stderr)
-    """
-    # Build command starting with 'git'
-    cmd = ['git'] + git_args
-    
-    logger.info(f"Executing git command: {' '.join(cmd)}")
-    
-    try:
-        result = subprocess.run(
-            cmd,
-            cwd=repo_dir,
-            capture_output=True,
-            text=True,
-            timeout=timeout,
-            check=False
-        )
-        return result.returncode, result.stdout, result.stderr
-    except subprocess.TimeoutExpired:
-        logger.error(f"Git command timed out after {timeout} seconds")
-        return 1, "", f"Git command timed out after {timeout} seconds"
-    except Exception as e:
-        logger.error(f"Error executing git command: {e}")
-        return 1, "", str(e)
-
-
-def create_safe_docker_script(
-    repo_url: str,
-    branch: str,
-    prompt: str,
-    model_cli: str,
-    github_username: Optional[str] = None
-) -> str:
-    """
-    Create a safe Docker container script with properly escaped values.
-    
-    Args:
-        repo_url: Validated repository URL
-        branch: Validated branch name
-        prompt: User prompt (will be escaped)
-        model_cli: Validated model CLI name ('claude' or 'codex')
-        github_username: Optional GitHub username
-        
-    Returns:
-        Safe shell script for Docker container execution
-    """
-    # Use shlex.quote for proper shell escaping
-    safe_repo_url = shlex.quote(repo_url)
-    safe_branch = shlex.quote(branch)
-    safe_prompt = shlex.quote(prompt)
-    safe_model = shlex.quote(model_cli)
-    
-    # Build script with properly escaped values
-    script = f'''#!/bin/bash
-set -e
-echo "Setting up repository..."
-
-# Clone repository with validated and escaped parameters
-git clone -b {safe_branch} {safe_repo_url} /workspace/repo
-cd /workspace/repo
-
-# Configure git
-git config user.email "claude-code@automation.com"
-git config user.name "Claude Code Automation"
-
-echo "üìã Will extract changes as patch for later PR creation..."
-echo "Starting {safe_model.upper()} Code with prompt..."
-
-# Create a temporary file with the prompt
-echo {safe_prompt} > /tmp/prompt.txt
-
-# Check which CLI tool to use based on model selection
-if [ {safe_model} = "codex" ]; then
-    echo "Using Codex (OpenAI Codex) CLI..."
-    
-    # Set environment variables for non-interactive mode
-    export CODEX_QUIET_MODE=1
-    
-    # Run Codex with the prompt
-    if command -v codex >/dev/null 2>&1; then
-        codex < /tmp/prompt.txt
-        CODEX_EXIT_CODE=$?
-        echo "Codex finished with exit code: $CODEX_EXIT_CODE"
-        
-        if [ $CODEX_EXIT_CODE -ne 0 ]; then
-            echo "ERROR: Codex failed with exit code $CODEX_EXIT_CODE"
-            exit $CODEX_EXIT_CODE
-        fi
-        
-        echo "‚úÖ Codex completed successfully"
-    else
-        echo "ERROR: codex command not found"
-        exit 1
-    fi
-else
-    echo "Using Claude CLI..."
-    
-    # Run Claude with the prompt
-    if [ -f /usr/local/bin/claude ]; then
-        # Use the official --print flag for non-interactive mode
-        cat /tmp/prompt.txt | node /usr/local/bin/claude --print --allowedTools "Edit,Bash"
-        CLAUDE_EXIT_CODE=$?
-        echo "Claude Code finished with exit code: $CLAUDE_EXIT_CODE"
-        
-        if [ $CLAUDE_EXIT_CODE -ne 0 ]; then
-            echo "ERROR: Claude Code failed with exit code $CLAUDE_EXIT_CODE"
-            exit $CLAUDE_EXIT_CODE
-        fi
-        
-        echo "‚úÖ Claude Code completed successfully"
-    else
-        echo "ERROR: claude command not found"
-        exit 1
-    fi
-fi
-
-# Extract changes for PR creation
-echo "üîç Checking for changes..."
-if git diff --quiet && git diff --cached --quiet; then
-    echo "‚ùå No changes detected after running {safe_model}"
-    exit 1
-fi
-
-# Stage all changes
-echo "üìù Staging all changes..."
-git add -A
-
-# Create commit with safe message
-echo "üíæ Creating commit..."'''
-    
-    # Add commit message handling
-    if github_username:
-        safe_username = shlex.quote(github_username)
-        commit_msg = f"{model_cli.capitalize()}: {prompt[:100]}"
-        safe_commit_msg = shlex.quote(commit_msg)
-        script += f'''
-git commit -m {safe_commit_msg}
-'''
-    else:
-        commit_msg = f"{model_cli.capitalize()}: Automated changes"
-        safe_commit_msg = shlex.quote(commit_msg)
-        script += f'''
-git commit -m {safe_commit_msg}
-'''
-    
-    script += '''
-# Generate patch and diff information
-echo "üì¶ Generating patch file..."
-git format-patch HEAD~1 --stdout > /tmp/changes.patch
-echo "=== PATCH START ==="
-cat /tmp/changes.patch
-echo "=== PATCH END ==="
-
-# Also get the diff for display
-echo "=== GIT DIFF START ==="
-git diff HEAD~1 HEAD
-echo "=== GIT DIFF END ==="
-
-# List changed files for reference
-echo "=== CHANGED FILES START ==="
-git diff --name-only HEAD~1 HEAD
-echo "=== CHANGED FILES END ==="
-
-# Get before/after content for merge view
-echo "=== FILE CHANGES START ==="
-for file in $(git diff --name-only HEAD~1 HEAD); do
-    echo "FILE: $file"
-    echo "=== BEFORE START ==="
-    git show HEAD~1:"$file" 2>/dev/null || echo "FILE_NOT_EXISTS"
-    echo "=== BEFORE END ==="
-    echo "=== AFTER START ==="
-    cat "$file" 2>/dev/null || echo "FILE_DELETED"
-    echo "=== AFTER END ==="
-    echo "=== FILE END ==="
-done
-echo "=== FILE CHANGES END ==="
-
-# Exit successfully
-echo "Container work completed successfully"
-exit 0
-'''
-    
-    return script
\ No newline at end of file
